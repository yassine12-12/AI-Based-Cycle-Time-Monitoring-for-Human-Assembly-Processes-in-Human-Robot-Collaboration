# AI-Based Cycle Time Monitoring for Human Assembly Processes in Human-Robot Collaboration

[![Python](https://img.shields.io/badge/Python-3.10+-3776AB?style=for-the-badge&logo=python&logoColor=white)](https://python.org)
[![YOLOv8](https://img.shields.io/badge/YOLOv8-Ultralytics-00FFFF?style=for-the-badge&logo=yolo&logoColor=white)](https://ultralytics.com)
[![MediaPipe](https://img.shields.io/badge/MediaPipe-Google-FF6D00?style=for-the-badge&logo=google&logoColor=white)](https://mediapipe.dev)
[![OpenCV](https://img.shields.io/badge/OpenCV-5C3EE8?style=for-the-badge&logo=opencv&logoColor=white)](https://opencv.org)
[![PyTorch](https://img.shields.io/badge/PyTorch-EE4C2C?style=for-the-badge&logo=pytorch&logoColor=white)](https://pytorch.org)
[![Streamlit](https://img.shields.io/badge/Streamlit-FF4B4B?style=for-the-badge&logo=streamlit&logoColor=white)](https://streamlit.io)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg?style=for-the-badge)](https://opensource.org/licenses/MIT)

## ğŸ“‹ Table of Contents
- [Project Overview](#-project-overview)
- [System Architecture](#-system-architecture)
- [Experimental Setup](#-experimental-setup)
- [Key Features](#-key-features)
- [Model Performance](#-model-performance)
- [Installation & Setup](#-installation--setup)
- [Usage Guide](#-usage-guide)
- [Results](#-results)
- [Contributing](#-contributing)

## ğŸ¯ Project Overview

![Workstation Setup](images/arbeitsstation.jpeg)

This project develops an AI-based system for monitoring and analyzing cycle times in human assembly processes, specifically designed for human-robot collaboration (HRC) environments. The system combines computer vision, deep learning, and real-time analysis to automatically track assembly tasks performed by humans working alongside Universal Robot UR5.

### ğŸ¯ Motivation

In modern manufacturing systems, human-robot collaboration (HRC) is gaining increasing importance as it combines the flexibility of human workers with the precision and endurance of robotic systems. In such work environments, monitoring and optimizing human activities is a central factor for increasing productivity and ensuring high quality standards. A key metric in this context is the so-called cycle time - the time span required to execute a defined process step.

Traditionally, cycle time recording in manual assembly processes is carried out using sensor-based tracking systems, which are often associated with considerable installation and maintenance effort. With the increasing use of AI-based perception systems in combination with cameras, new possibilities are opening up for automated cycle time monitoring - even in dynamic and collaborative work environments.

### ğŸ”¬ Research Objectives

- **Literature research and requirements analysis** for HRC environments
- **Object detection** using custom YOLO model for assembly components
- **Human pose estimation** using MediaPipe for movement tracking
- **Automatic segmentation** of assembly phases (grasping, joining, screwing)
- **Cycle time calculation** per step using event-based triggering
- **Testing and validation** in real-world industrial scenarios

## ğŸ—ï¸ System Architecture

![System Flowchart](images/flowchart.png)

The system architecture consists of several integrated components:

### ğŸ“· Vision System
- **Dual camera setup** with main camera and container monitoring
- **Basler industrial cameras** for high-quality image acquisition
- **Real-time video processing** with optimized frame rates

![Main Camera View](images/Hauptkamera.png) ![Container Monitoring](images/Kiste.png)

### ğŸ§  AI Processing Pipeline
1. **YOLOv8 Object Detection** - Custom trained model for assembly components
2. **MediaPipe Hand Tracking** - Real-time hand pose estimation
3. **Step Logic Engine** - Intelligent assembly phase detection
4. **Timer Management** - Precise cycle time measurement

## ğŸš€ Key Features

### ğŸ¤– AI-Powered Detection
- **Custom YOLOv8 model** trained on assembly components (Motor, LModul, Uno, Erweiterungsboard, etc.)
- **Real-time object detection** with confidence scoring and bounding box visualization
- **MediaPipe hand tracking** for precise human gesture recognition
- **Multi-camera support** with synchronized video analysis

### â±ï¸ Intelligent Cycle Time Monitoring
- **Automated step detection** based on hand-object proximity algorithms
- **Multiple completion conditions**: distance-based, timeout-based, ROI-based
- **Real-time progress tracking** with visual feedback and status indicators
- **CSV export** of detailed timing data for comprehensive analysis

### ğŸ›ï¸ Advanced Configuration System
- **Modular architecture** with component-based design for easy maintenance
- **Configurable assembly steps** with custom parameters and thresholds
- **ROI (Region of Interest)** support for focused monitoring zones
- **Detection smoothing** algorithms to reduce noise and false positives

### ğŸ”§ Industrial Integration
- **Basler camera support** for industrial-grade imaging solutions
- **Real-time processing** optimized for production environments
- **Container monitoring** (Kiste) for material flow tracking
- **ArUco marker detection** for additional spatial reference tracking

## ğŸ“Š Model Performance

### YOLOv8 Training Results

![Confusion Matrix](images/confusion_matrix.png)

The custom YOLOv8 model was trained on assembly components with the following performance metrics:

![PR Curve](images/PR_curve.png)

**Model Specifications:**
- **Architecture**: YOLOv8n (nano) for optimal real-time performance
- **Training Dataset**: Custom annotated assembly components
- **Classes**: Motor, LModul, Uno, Erweiterungsboard, and other assembly parts
- **Model File**: `best_jasin.pt` (included in repository)

**Performance Metrics:**
- High precision and recall across all object classes
- Optimized for real-time detection in industrial environments
- Robust performance under varying lighting conditions

## ğŸ› ï¸ Installation & Setup

### Prerequisites
- **Python 3.10+** (recommended: Python 3.11)
- **Windows 10/11** (tested environment)
- **USB Camera or Basler Industrial Camera**
- **Minimum 8GB RAM** for optimal performance

### Step 1: Clone Repository
```powershell
git clone https://github.com/yassine12-12/AI-Based-Cycle-Time-Monitoring-for-Human-Assembly-Processes-in-Human-Robot-Collaboration.git
cd "AI-based cycle time monitoring for human assembly processes in human-robot collaboration"
```

### Step 2: Create Virtual Environment
```powershell
python -m venv .venv
.venv\Scripts\activate
```

### Step 3: Install Dependencies
```powershell
pip install -r requirements.txt
```

### Step 4: Verify Installation
```powershell
## ğŸ“š Usage Guide

### ğŸ¥ Real-Time Hand and Object Detection

**Using Python Script:**
```powershell
# Activate virtual environment
.venv\Scripts\activate

# Run with default camera (index 0)
python modular/run_modular.py

# Run with specific camera
python modular/run_modular.py --camera 1
```

**Using Batch File (Windows):**
Simply double-click `run_yolo_mediapipe.bat` for quick startup.

### ğŸ“¹ Video Recording for Analysis

**Record Raw Video:**
```powershell
.venv\Scripts\activate
python src/raw_video_capture.py
```
- Press `q` to stop recording
- Videos are automatically saved in `videos/` folder with timestamp
- The `videos/` folder is git-ignored for privacy

### ğŸ” Assembly Analysis and Cycle Time Measurement

**Analyze Pre-recorded Videos:**
```powershell
.venv\Scripts\activate
python modular/components/main_app.py
```

**Video Control During Analysis:**
- `q` - Quit analysis
- `Space` - Pause/Resume
- `a`/`d` - Frame backward/forward
- `w`/`s` - Fast backward/forward navigation

### ğŸ–¼ï¸ Extract Training Frames

**Extract frames from videos for annotation:**
```powershell
python src/extract_frames.py
```
- Extracts every 10th frame by default (configurable via `FRAME_SKIP`)
- Output saved to `videos/frames/` in separate folders per video
- Ideal for creating training datasets for Roboflow or similar annotation tools

## ğŸ› Troubleshooting

### Common Issues and Solutions

**Camera Not Detected:**
```powershell
# Check available cameras
python -c "import cv2; print([i for i in range(10) if cv2.VideoCapture(i).read()[0]])"
```

**Performance Issues:**
- Reduce camera resolution in config
- Lower YOLO confidence threshold
- Close unnecessary applications
- Ensure adequate RAM availability

**Model Loading Errors:**
```powershell
# Verify model file integrity
python -c "from ultralytics import YOLO; model = YOLO('best_jasin.pt'); print('Model loaded successfully')"
```

**Dependencies Missing:**
```powershell
# Reinstall requirements
.venv\Scripts\activate
pip install --upgrade -r requirements.txt
```

**Basler Camera Issues:**
- Install Basler Pylon SDK
- Check camera drivers
- Verify USB 3.0 connection
- Review camera permissions

## ğŸ¤ Contributing

We welcome contributions to improve the system! Here's how you can help:

### Development Setup
1. Fork the repository
2. Create a feature branch: `git checkout -b feature-name`
3. Make your changes and test thoroughly
4. Submit a pull request with detailed description

### Areas for Contribution
- **Algorithm improvements** for better detection accuracy
- **Performance optimizations** for real-time processing
- **Additional assembly step templates** for different scenarios
- **Documentation enhancements** and tutorials
- **Bug fixes** and stability improvements

### Code Style
- Follow PEP 8 guidelines for Python code
- Add comprehensive docstrings for new functions
- Include unit tests for new features
- Update documentation for any API changes

## ï¿½ Contact

For questions, suggestions, or collaboration opportunities:

- **Project Repository**: [GitHub Repository](https://github.com/yassine12-12/AI-Based-Cycle-Time-Monitoring-for-Human-Assembly-Processes-in-Human-Robot-Collaboration)
- **Research Group**: Technical University of Berlin - Manufacturing Technology
- **Issues & Support**: Please use the GitHub Issues page for bug reports and feature requests

---

**Note**: This system is designed for research and educational purposes. For industrial deployment, additional safety measures and validation may be required.

### Experimental Validation

The system has been tested in real-world assembly scenarios with the following results:

**Cycle Time Accuracy:**
- Â±2-3% deviation from manual timing measurements
- Consistent performance across different operators
- Reliable detection under varying lighting conditions

**Detection Performance:**
- 95%+ object detection accuracy for trained components
- Real-time processing at 30+ FPS on standard hardware
- Robust hand tracking with MediaPipe integration

**System Reliability:**
- Stable operation during extended testing periods
- Minimal false positives through detection smoothing
- Effective handling of occlusions and partial object visibility
## ğŸ—‚ï¸ Project Structure

```
â”œâ”€â”€ modular/                          # Main application modules
â”‚   â”œâ”€â”€ components/                   # Core system components
â”‚   â”‚   â”œâ”€â”€ config.py                # Configuration management
â”‚   â”‚   â”œâ”€â”€ main_app.py              # Main application logic
â”‚   â”‚   â”œâ”€â”€ step_logic.py            # Assembly step detection
â”‚   â”‚   â”œâ”€â”€ timer_utils.py           # Cycle time measurement
â”‚   â”‚   â”œâ”€â”€ vision_utils.py          # Computer vision utilities
â”‚   â”‚   â”œâ”€â”€ vibration_utils.py       # Vibration analysis
â”‚   â”‚   â””â”€â”€ kiste_utils.py           # Container monitoring
â”‚   â””â”€â”€ run_modular.py               # Application entry point
â”œâ”€â”€ yolov8 training/                 # Model training resources
â”‚   â”œâ”€â”€ train_yolo.ipynb            # Training notebook
â”‚   â”œâ”€â”€ results.csv                 # Training metrics
â”‚   â””â”€â”€ runs/                       # Training outputs
â”œâ”€â”€ images/                         # Documentation images
â”‚   â”œâ”€â”€ arbeitsstation.jpeg         # Workstation setup
â”‚   â”œâ”€â”€ flowchart.png              # System architecture
â”‚   â”œâ”€â”€ confusion_matrix.png       # Model performance
â”‚   â””â”€â”€ ...
â”œâ”€â”€ best_jasin.pt                   # Trained YOLOv8 model
â”œâ”€â”€ requirements.txt                # Python dependencies
â””â”€â”€ README.md                       # Project documentation
```

## âš™ï¸ Configuration

### Camera Setup
The system supports both USB webcams and industrial Basler cameras. Configure camera settings in `modular/components/config.py`:

```python
# Camera configuration
CAMERA_INDEX = 0  # USB camera index
BASLER_ENABLED = False  # Set to True for Basler cameras
RESOLUTION = (1920, 1080)  # Camera resolution
```

### Assembly Steps Configuration
Define custom assembly steps and their detection parameters:

```python
# Assembly step definitions
ASSEMBLY_STEPS = {
    "step1": {
        "name": "Grasp Motor",
        "target_objects": ["Motor"],
        "completion_conditions": ["distance", "timeout"],
        "parameters": {...}
    },
    # Additional steps...
}
```

### Detection Thresholds
Adjust detection sensitivity and performance:

```python
# YOLO detection settings
CONFIDENCE_THRESHOLD = 0.7
NMS_THRESHOLD = 0.45

# MediaPipe hand tracking
HAND_CONFIDENCE = 0.8
HAND_TRACKING_CONFIDENCE = 0.5
```
## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- **Technical University of Berlin** - Research support and facilities
- **Ultralytics** - YOLOv8 framework and documentation
- **Google MediaPipe** - Hand tracking and pose estimation
- **OpenCV Community** - Computer vision tools and libraries


